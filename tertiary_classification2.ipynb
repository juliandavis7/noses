{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tertiary_classification2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s-DEkoa2PyT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoIXF2Yj192G",
        "outputId": "0a895814-924c-468e-80ca-f9e1e8576951"
      },
      "source": [
        "# standard\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# cnn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# results \n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# file system\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_data = \"/content/drive/MyDrive/NOSES/cnn_data/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFmrrnL2Uxg"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PomzbKV2DHW"
      },
      "source": [
        "def preprocessing(X_init, y_init):\n",
        "  X_init = X_init[:] / 255.\n",
        "  y_init = y_init[:] / 1.\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  for Xi, yi in zip(X_init, y_init):\n",
        "    im = tf.image.resize(Xi, (96, 96))\n",
        "    X.append(im)\n",
        "    y.append(yi)\n",
        "\n",
        "  X = tf.stack(X)\n",
        "  y = tf.stack(y)\n",
        "  return X, y\n",
        "\n",
        "# given X and y data, returns newly shuffled X and y data\n",
        "def shuffle_xy(X, y):\n",
        "  indices = tf.range(start=0, limit=tf.shape(Xtrain)[0], dtype=tf.int32)\n",
        "  shuffled_indices = tf.random.shuffle(indices)\n",
        "  X_shuffled = tf.gather(X, shuffled_indices)\n",
        "  y_shuffled = tf.gather(y, shuffled_indices)\n",
        "  return X_shuffled, y_shuffled"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meYrNF3A2KPG"
      },
      "source": [
        "## Load in images and bounding box meta data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ9MHy2l2HQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f3b4b3-a292-45a9-dde3-a0c28d80f2d1"
      },
      "source": [
        "imgs = np.load(path_data + \"images.npy\", allow_pickle=True)\n",
        "bb_data = np.load(path_data + \"bb_data.npy\", allow_pickle=True)\n",
        "\n",
        "imgs.shape, bb_data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((125610, 150, 150, 3), (125610,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrOoAoBlBBhg"
      },
      "source": [
        "### Extract tertiary labels\n",
        "- 0 for \"no seal\"\n",
        "- 1 for \"partial seal\"\n",
        "- 2 for \"full seal\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-0rIDEPAY2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8da71c-56e0-4beb-89ab-064e388f87c6"
      },
      "source": [
        "def get_seal_percent(df_subimg):\n",
        "  seal_percent = 0\n",
        "  for i in df_subimg.itertuples():\n",
        "    #print(i)\n",
        "    seal_percent += i[3] # have to add for cases where multiple seals are present\n",
        "  return seal_percent\n",
        "\n",
        "def get_labels(bb_data, threshold = 1):\n",
        "  labels = []\n",
        "  for df_subimg in bb_data:\n",
        "    if not isinstance(df_subimg, type(None)):\n",
        "      seal_percent = get_seal_percent(df_subimg)\n",
        "      if seal_percent > threshold:\n",
        "        val = 2\n",
        "      else:\n",
        "        val = 1\n",
        "    else:\n",
        "      val = 0\n",
        "    labels.append(val)\n",
        "  labels = np.array(labels)\n",
        "  return labels\n",
        "\n",
        "labels = get_labels(bb_data)\n",
        "# TODO: ask Dr. Dekyhtar how to handle situation where two seals are present in a subimage\n",
        "\n",
        "pd.Series(labels).value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    107925\n",
              "1     10496\n",
              "2      7189\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDHILkIx2iam"
      },
      "source": [
        "## Preprocessing and Train-Test Split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1gB6i7skCTP",
        "outputId": "0fc08757-b01b-417b-9dfb-541ab0ebd488"
      },
      "source": [
        "def get_xy_indices(bb_data, test_frac = .1):\n",
        "  # get indices for images w seals\n",
        "  indices_w_seal = []\n",
        "  for i in range(len(bb_data)):\n",
        "    if not bb_data[i] is None:\n",
        "      indices_w_seal.append(i)\n",
        "\n",
        "  num_w_seal = len(indices_w_seal)\n",
        "  size_dataset = int(num_w_seal * 10/4) # size for dataset with 40% of images containing seals\n",
        "  num_wo_seal = size_dataset - num_w_seal\n",
        "\n",
        "  # get indices for images w/o seals\n",
        "  all_indices_wo_seal = [x for x in list(range(len(bb_data))) if x not in indices_w_seal]\n",
        "  indices_wo_seal = random.sample(all_indices_wo_seal, num_wo_seal)\n",
        "\n",
        "  print(\"num sub images for dataset with 40% images containing seals\", size_dataset)\n",
        "  print(\"num sub images with a seal\", num_w_seal)\n",
        "  print(\"num sub images without a seal\", num_wo_seal)\n",
        "\n",
        "  num_train_w_seal = round((1 - test_frac) * num_w_seal)\n",
        "  num_train_wo_seal = round((1 - test_frac) * num_wo_seal)\n",
        "  \n",
        "  # train test split of indices, each with 40% of sub imgs containing seals\n",
        "  train_indices = np.concatenate((np.array(indices_w_seal[:num_train_w_seal]), \n",
        "                                  np.array(indices_wo_seal[:num_train_wo_seal])))\n",
        "  test_indices = np.concatenate((np.array(indices_w_seal[num_train_w_seal:]), \n",
        "                                  np.array(indices_wo_seal[num_train_wo_seal:])))\n",
        "  return train_indices, test_indices\n",
        "\n",
        "train_indices, test_indices = get_xy_indices(bb_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num sub images for dataset with 40% images containing seals 44212\n",
            "num sub images with a seal 17685\n",
            "num sub images without a seal 26527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIx5eZHR2kk4"
      },
      "source": [
        "Xtrain, ytrain = preprocessing(imgs[train_indices], labels[train_indices])\n",
        "Xtest, ytest = preprocessing(imgs[test_indices], labels[test_indices])\n",
        "\n",
        "print(\"train size:\", len(Xtrain), len(ytrain))\n",
        "print(\"test size:\", len(Xtest), len(ytest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WotcYzTh3SAq"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7VoogZU_tmh"
      },
      "source": [
        "lr = 3e-4\r\n",
        "batch_size = 32\r\n",
        "conv_dim_init = 64\r\n",
        "epochs = 1\r\n",
        "\r\n",
        "def create_model(cnn_blocks=1, dense_layers=1, filter_multiplier = 1, \r\n",
        "                 kernel_size=3, strides=(1, 1), dense_output_size =1024):\r\n",
        "  model = keras.models.Sequential()\r\n",
        "  for i in range(cnn_blocks):\r\n",
        "    conv_output_dim = (conv_dim_init * filter_multiplier) * (i + 1)\r\n",
        "    model.add(layers.Conv2D(filters=conv_output_dim, kernel_size=kernel_size, \r\n",
        "                            strides=strides, activation='relu',padding='same'))\r\n",
        "    model.add(layers.Conv2D(filters=conv_output_dim, kernel_size=kernel_size, \r\n",
        "                            strides=strides, activation='relu',padding='same'))\r\n",
        "    model.add(layers.MaxPooling2D(2, 2))\r\n",
        "  model.add(layers.Flatten())\r\n",
        "  for i in range(dense_layers):\r\n",
        "    model.add(layers.Dense(units=dense_output_size , activation='relu'))\r\n",
        "  model.add(layers.Dense(3, activation='softmax',name='z'))\r\n",
        "\r\n",
        "  opt = tf.keras.optimizers.Adam(lr=lr) #sgd\r\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics='accuracy')\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "def run_model(cnn_blocks, dense_layers, filter_multiplier, \r\n",
        "              kernel_size, strides, dense_output_size):\r\n",
        "  model = create_model(cnn_blocks, dense_layers, filter_multiplier, kernel_size, strides, dense_output_size)\r\n",
        "  model.fit(Xtrain, ytrain, batch_size=batch_size, epochs=epochs,\r\n",
        "            validation_split=.1, verbose=True)\r\n",
        "  res = model.evaluate(Xtest, ytest); loss = res[0]; acc = res[1]\r\n",
        "  ypred = model.predict(Xtest)\r\n",
        "  print(\"%d CCP Block(s), %d Dense Layer(s), %dx Filter Multiplier, %d Kernel Size, %s Strides, %d Dense Output Size\" % \r\n",
        "        (cnn_blocks, dense_layers, filter_multiplier, kernel_size, strides, dense_output_size),\r\n",
        "        \"\\nLoss:       %f\\nAccuracy:   %f\" % (loss, acc))\r\n",
        "  return ypred\r\n",
        "\r\n",
        "# convert ypred back to predictions of 0, 1, 2\r\n",
        "# convert ytest from tensor to list\r\n",
        "def convert_arrs(ypred, ytest):\r\n",
        "  ypred_ = []\r\n",
        "  for i in range(len(ypred)):\r\n",
        "    confidence_arr = list(ypred[i])\r\n",
        "    ypred_.append(np.argmax(confidence_arr))\r\n",
        "  ytest_ = []\r\n",
        "  for i in range(len(ytest)):\r\n",
        "    ytest_.append(int(ytest[i]))\r\n",
        "  return ypred_, ytest_\r\n",
        "\r\n",
        "def print_metrics(ypred, ytest):\r\n",
        "  ypred_, ytest_ = convert_arrs(ypred, ytest)\r\n",
        "  f1 = f1_score(np.array(ytest_), ypred_, labels=np.unique(ytest_), average=\"weighted\")\r\n",
        "  precision = precision_score(np.array(ytest_), ypred_, labels=np.unique(ypred_), average=\"weighted\")\r\n",
        "  recall = recall_score(np.array(ytest_), ypred_, labels=np.unique(ypred_), average=\"weighted\")\r\n",
        "  print(\"F1 Score:  \", f1)\r\n",
        "  print(\"Precision: \", precision)\r\n",
        "  print(\"Recall:    \", recall)\r\n",
        "  print(\"Confusion Matrix:\")\r\n",
        "  print(pd.crosstab(pd.Series(ytest_), pd.Series(ypred_), rownames=['Actual'], colnames=['Predicted'], margins=True), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4k3XxrZ3MsS"
      },
      "source": [
        "## Grid Search\n",
        "Paramterers: \n",
        "- Number of Conv-Conv-Pool (CCP) blocks\n",
        "- Number of Dense layers\n",
        "- Kernel Size: specifies height and width of convolution window\n",
        "- Strides: specifies the strides of the convolution along the height and width\n",
        "- Dense Ouput Size: size of output space for the dense layer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy1R7DSLYGgr"
      },
      "source": [
        "cnn_blocks_grid = [1, 2, 3]\n",
        "dense_layers_grid = [1, 2, 3]\n",
        "filter_multiplier_grid  = [.5, 1, 2]\n",
        "kernel_size_grid = [2, 3, 4]\n",
        "strides_grid = [(1, 1)] = [(1, 1), (2, 2), (3, 3)]\n",
        "dense_output_size_grid = [1024, 2048, 4096]\n",
        "\n",
        "for cnn_blocks in cnn_blocks_grid:\n",
        "  for dense_layers in dense_layers_grid:\n",
        "    for filter_multiplier in filter_multiplier_grid:\n",
        "      for kernel_size in kernel_size_grid:\n",
        "        for strides in strides_grid:\n",
        "          for dense_output_size in dense_output_size_grid:\n",
        "            ypred = run_model(cnn_blocks, dense_layers, filter_multiplier, \n",
        "                      kernel_size, strides, dense_output_size)\n",
        "            print_metrics(ypred, ytest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h1VLGJ0TQb6"
      },
      "source": [
        "\n",
        "\n",
        "print(len(ypred_))\n",
        "print(len(ytest_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzeWwH8lwdgn"
      },
      "source": [
        "# check accuracy\n",
        "count = 0\n",
        "length = len(ypred_)\n",
        "for i in range(length):\n",
        "  if ypred_[i] == ytest_[i]:\n",
        "    count += 1\n",
        "\n",
        "acc = count/length\n",
        "print(\"Model Accuracy: \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aXcEaRf_BB0"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JKeqluE-_-u"
      },
      "source": [
        "def get_metrics(ypred_, ytest)\r\n",
        "  print(\"F1 Score: \", f1_score(np.array(ytest_), ypred_, labels=np.unique(ytest_), average=\"weighted\"))\r\n",
        "  print(\"Precision: \",recall_score(np.array(ytest_), ypred_, labels=np.unique(ypred_), average=\"weighted\"))\r\n",
        "  print(\"Recall: \",precision_score(np.array(ytest_), ypred_, labels=np.unique(ypred_), average=\"weighted\"), \"\\n\")\r\n",
        "  print(\"Confusion Matrix:\")\r\n",
        "  print(pd.crosstab(pd.Series(ytest_), pd.Series(ypred_), rownames=['Actual'], colnames=['Predicted'], margins=True), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQocHXmTK8_m"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss over time')\n",
        "plt.legend(['train','val'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('acc')\n",
        "plt.title('Accuracy over time')\n",
        "plt.legend(['train','val'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}